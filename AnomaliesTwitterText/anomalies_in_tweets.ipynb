{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### \"Detection of anomalous tweets using supervising outlier techniques\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Dependencies and Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"original_train_data.csv\", header = None,delimiter = \"\\t\", quoting=3,names = [\"Polarity\",\"TextFeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>TextFeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polarity                                           TextFeed\n",
       "0         1            The Da Vinci Code book is just awesome.\n",
       "1         1  this was the first clive cussler i've ever rea...\n",
       "2         1                   i liked the Da Vinci Code a lot.\n",
       "3         1                   i liked the Da Vinci Code a lot.\n",
       "4         1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Visualization\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prepration with the available data. I made the combination such that the classes are highly imbalanced making it apt for anomaly detection problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3995\n",
       "0      20\n",
       "Name: Polarity, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_positive = data.loc[data[\"Polarity\"]==1]\n",
    "data_negative = data.loc[data[\"Polarity\"]==0]\n",
    "anomaly_data = pd.concat([data_negative.sample(n=10),data_positive,data_negative.sample(n=10)])\n",
    "anomaly_data.Polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words for sentence in train data 10.5379825654\n"
     ]
    }
   ],
   "source": [
    "#Number of words per sentence\n",
    "print (\"No of words for sentence in train data\",np.mean([len(s.split(\" \")) for s in anomaly_data.TextFeed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing - text analytics to create a corpus\n",
    " \n",
    "    1) Converting text to matrix of token counts [Bag of words]\n",
    "          Stemming -  lowercasing, removing stop-words, removing punctuation and reducing words to its lexical roots \n",
    "    2) Stemmer, tokenizer(removes non-letters) are created by ourselves.These are passed as parameters to CountVectorizer of sklearn.\n",
    "    3) Extracting important words and using them as input to the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download()\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' this code is taken from\n",
    "http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "'''\n",
    "# a stemmer widely used\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "Note: I am not using frequencies(TfidTransformer, apt for longer documents) because the text size is small and can be dealt with occurences(CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Max_Features selected as 80 - can be changed for the better trade-off\n",
    "vector_data = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 90\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_Transform:\n",
    "    1) Fits the model and learns the vocabulary\n",
    "    2) transoforms the data into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4015, 90)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using only the \"Text Feed\" column to build the features\n",
    "features = vector_data.fit_transform(anomaly_data.TextFeed.tolist())\n",
    "#converting the data into the array\n",
    "features = features.toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolut', 'accept', 'anyon', 'awesom', 'beauti', 'becaus', 'becom', 'bitch', 'bonker', 'book', 'brokeback', 'care', 'catcher', 'code', 'commun', 'count', 'coz', 'da', 'dash', 'deep', 'desper', 'differ', 'don', 'dudee', 'escapad', 'excel', 'felicia', 'film', 'freakin', 'friend', 'fun', 'gon', 'good', 'got', 'grab', 'great', 'harri', 'hill', 'homosexu', 'imposs', 'jane', 'join', 'kate', 'key', 'know', 'leah', 'like', 'love', 'lubb', 'luv', 'make', 'man', 'mission', 'mom', 'mountain', 'movi', 'na', 'peopl', 'place', 'potter', 'read', 'realli', 'right', 'rock', 's', 'said', 'say', 'sentri', 'seri', 'silent', 'stand', 'start', 'stori', 't', 'thi', 'thing', 'think', 'thought', 'tom', 'turn', 'tye', 'vinci', 'virgin', 'wa', 'wait', 'want', 'watch', 'whi', 'worth', 'yeah']\n"
     ]
    }
   ],
   "source": [
    "#printing the words in the vocabulary\n",
    "vocab = vector_data.get_feature_names()\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absolut', 93), ('accept', 81), ('anyon', 81), ('awesom', 1129), ('beauti', 128), ('becaus', 343), ('becom', 81), ('bitch', 80), ('bonker', 150), ('book', 1009), ('brokeback', 82), ('care', 80), ('catcher', 1016), ('code', 82), ('commun', 81), ('count', 80), ('coz', 1014), ('da', 81), ('dash', 84), ('deep', 89), ('desper', 80), ('differ', 80), ('don', 86), ('dudee', 80), ('escapad', 160), ('excel', 92), ('felicia', 81), ('film', 85), ('freakin', 81), ('friend', 80), ('fun', 80), ('gon', 112), ('good', 92), ('got', 80), ('grab', 92), ('great', 1092), ('harri', 80), ('hill', 81), ('homosexu', 80), ('imposs', 80), ('jane', 1000), ('join', 80), ('kate', 80), ('key', 176), ('know', 1050), ('leah', 1873), ('like', 82), ('love', 90), ('lubb', 82), ('luv', 1000), ('make', 83), ('man', 1008), ('mission', 426), ('mom', 83), ('mountain', 167), ('movi', 83), ('na', 1092), ('peopl', 200), ('place', 188), ('potter', 88), ('read', 87), ('realli', 365), ('right', 82), ('rock', 93), ('s', 80), ('said', 177), ('say', 80), ('sentri', 81), ('seri', 161), ('silent', 170), ('stand', 187), ('start', 101), ('stori', 91), ('t', 95), ('thi', 89), ('thing', 95), ('think', 82), ('thought', 1014), ('tom', 844), ('turn', 89), ('tye', 253), ('vinci', 103), ('virgin', 168), ('wa', 82), ('wait', 166)]\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "    \n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the data set\n",
    "a = zip(vocab,dist)\n",
    "print (list(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#80:20 ratio\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features, \n",
    "        anomaly_data.Polarity,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data - positive and negative values\n",
      "1    3196\n",
      "0      16\n",
      "Name: Polarity, dtype: int64\n",
      "Testing data - positive and negative values\n",
      "1    799\n",
      "0      4\n",
      "Name: Polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data - positive and negative values\")\n",
    "print (pd.value_counts(pd.Series(y_train)))\n",
    "print (\"Testing data - positive and negative values\")\n",
    "print (pd.value_counts(pd.Series(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text polarity depends on what words appear in that text, discarding any grammar or word order but keeping multiplicity.\n",
    "\n",
    "1) All the above text processing for features ended up with the same entries in our dataset\n",
    "\n",
    "2) Instead of having them defined by a whole text, they are now defined by a series of counts of the most frequent words in our whole corpus.\n",
    "\n",
    "3) These vectors are used as features to train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight={0: 20}, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X=X_train,y=y_train)\n",
    "\n",
    "wclf = SVC(class_weight={0: 20})\n",
    "wclf.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_weighted = wclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVM metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       1.00      1.00      1.00       799\n",
      "\n",
      "avg / total       0.99      1.00      0.99       803\n",
      "\n",
      "Weighted SVM metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      1.00      0.40         4\n",
      "          1       1.00      0.98      0.99       799\n",
      "\n",
      "avg / total       1.00      0.99      0.99       803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manojkumar_meno\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (\"Basic SVM metrics\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print (\"Weighted SVM metrics\")\n",
    "print(classification_report(y_test, y_pred_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVM Confusion Matrix\n",
      "[[  0   4]\n",
      " [  0 799]]\n",
      "Weighted SVM Confusion Matrix\n",
      "[[  4   0]\n",
      " [ 12 787]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print (\"Basic SVM Confusion Matrix\")\n",
    "print (confusion_matrix(y_test, y_pred))\n",
    "print (\"Weighted SVM Confusion Matrix\")\n",
    "print (confusion_matrix(y_test, y_pred_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0, 12, 787)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_weighted).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "    \n",
    "    As seen from the above procedure, we have to perform cost-sensitive learning using weighting methods (adding more weight to anomaly classes) to deal with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
